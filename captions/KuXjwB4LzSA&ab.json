{"text": "Suppose I give you two different lists of\nnumbers or maybe two different functions and", "start": 0.13, "duration": 4.68}
{"text": "I ask you to think of all the ways you might\ncombine those two lists to get a new list", "start": 4.81, "duration": 3.53}
{"text": "of numbers or combine the two functions to\nget a new function.", "start": 8.34, "duration": 3.94}
{"text": "Maybe one simple way that comes to mind is\nto simply add them together term by term.", "start": 12.28, "duration": 4.46}
{"text": "Likewise, with the functions, you can add\nall the corresponding outputs.", "start": 16.74, "duration": 3.92}
{"text": "In a similar vein, you could also multiply\nthe two lists term by term and do the same", "start": 20.66, "duration": 4.08}
{"text": "thing with the functions.", "start": 24.74, "duration": 1.76}
{"text": "But there's another kind of combination just\nas fundamental as both of those but a lot", "start": 26.5, "duration": 4.53}
{"text": "less commonly discussed known as a Convolution.", "start": 31.03, "duration": 3.119}
{"text": "But unlike the previous two cases, it's not\nsomething that's merely inherited from an", "start": 34.149, "duration": 4.111}
{"text": "operation you can do to numbers.", "start": 38.26, "duration": 2.139}
{"text": "It's something genuinely new for the context\nof list of numbers or combining functions.", "start": 40.399, "duration": 4.701}
{"text": "They show up all over the place, they are\nubiquitous in image processing.", "start": 45.1, "duration": 3.65}
{"text": "It's a core construct in the theory of probability,\nthey're used a lot in solving differential", "start": 48.75, "duration": 5.039}
{"text": "equations and one context where you've almost\ncertainly seen it if not by this name is multiplying", "start": 53.789, "duration": 5.081}
{"text": "to polynomials together.", "start": 58.87, "duration": 2.009}
{"text": "As someone in the business of visual explanations,\nthis is an especially great topic.", "start": 60.879, "duration": 4.411}
{"text": "Because the formulaic definition in isolation\nand without context can look kind of intimidating.", "start": 65.29, "duration": 5.35}
{"text": "But if we take the time to really unpack what\nit's saying and before that, actually motivate", "start": 70.64, "duration": 4.22}
{"text": "why you would want something like this, it's\nan incredibly beautiful operation.", "start": 74.86, "duration": 4.25}
{"text": "And I have to admit I actually learned a little\nsomething while putting together the visuals", "start": 79.11, "duration": 3.63}
{"text": "for this project.", "start": 82.74, "duration": 1.34}
{"text": "In the case of convolving two different functions,\nI was trying to think of different ways you", "start": 84.08, "duration": 3.53}
{"text": "might picture what that mean.", "start": 87.61, "duration": 2.03}
{"text": "And with one of them, I had a little bit of\nan aha moment for why it is that normal distributions", "start": 89.64, "duration": 5.18}
{"text": "play the role that they do in probability.", "start": 94.82, "duration": 1.89}
{"text": "Why it's such a natural shape for a function\nbut I'm getting ahead of myself, there's a", "start": 96.71, "duration": 3.76}
{"text": "lot of setup for that one.", "start": 100.47, "duration": 1.46}
{"text": "In this video, our primary focus is just going\nto be on the discrete case and in particular", "start": 101.93, "duration": 4.32}
{"text": "building up to a very unexpected but very\nclever algorithm for computing these, and", "start": 106.25, "duration": 4.96}
{"text": "I'll pull out the discussion for the continuous\ncase into a second part.", "start": 111.21, "duration": 7.76}
{"text": "It's very tempting to open up with the image\nprocessing examples since they're visually", "start": 118.97, "duration": 3.81}
{"text": "the most intriguing but there are couple bits\nof finickiness that make the image processing", "start": 122.78, "duration": 3.68}
{"text": "case less representative of convolutions overall.", "start": 126.46, "duration": 2.73}
{"text": "So instead, let's kick things off with probability.", "start": 129.19, "duration": 2.87}
{"text": "And in particular, one of the simplest examples\nthat I'm sure everyone hears thought about", "start": 132.06, "duration": 3.47}
{"text": "at some point in their life.", "start": 135.53, "duration": 1.47}
{"text": "Which is rolling a pair of dice and figuring\nout the chances of seeing various different", "start": 137.0, "duration": 4.31}
{"text": "sums.", "start": 141.31, "duration": 1.0}
{"text": "And you might say not a problem.", "start": 142.31, "duration": 1.47}
{"text": "Not a problem.", "start": 143.78, "duration": 1.0}
{"text": "Each of your two dice has six different possible\noutcomes, which gives us a total of 36 distinct", "start": 144.78, "duration": 5.0}
{"text": "possible pairs of outcomes.", "start": 149.78, "duration": 2.15}
{"text": "And if we just look through them all, we can\ncount up how many pairs have a given sum.", "start": 151.93, "duration": 4.67}
{"text": "And arranging all the pairs in a grid like\nthis, one pretty nice thing is that all of", "start": 156.6, "duration": 4.2}
{"text": "the pairs that have a constant sum are visible\nalong one of these different diagonals.", "start": 160.8, "duration": 5.23}
{"text": "So simply counting how many exist on each\nof those diagonals will tell you how likely", "start": 166.03, "duration": 4.39}
{"text": "you are to see a particular sum.", "start": 170.42, "duration": 1.95}
{"text": "And I'd say, very good, very good.", "start": 172.37, "duration": 2.82}
{"text": "But can you think of any other ways that you\nmight visualize the same question?", "start": 175.19, "duration": 4.269}
{"text": "Other images that can come to mind, to think\nof all the distinct pairs that have a given", "start": 179.459, "duration": 4.36}
{"text": "sum?", "start": 183.819, "duration": 1.28}
{"text": "And maybe one of you raises your hand and\nsays, 'Yeah, I've got one.'", "start": 185.099, "duration": 3.041}
{"text": "Let's say you picture these two different\nsets of possibilities each in a row, but you", "start": 188.14, "duration": 4.15}
{"text": "flip around that second row.", "start": 192.29, "duration": 1.949}
{"text": "That way, all of the different pairs which\nadd up to seven line up vertically like this...", "start": 194.239, "duration": 5.39}
{"text": "And if we slide that bottom row all the way\nto the right, then the unique pair that adds", "start": 199.629, "duration": 3.731}
{"text": "up to two, the snake eyes, are the only ones\nthat align.", "start": 203.36, "duration": 3.45}
{"text": "And if I shlunk that over one unit to the\nright, the pairs which align are the two different", "start": 206.81, "duration": 4.319}
{"text": "pairs that add up to three.", "start": 211.129, "duration": 2.03}
{"text": "And in general, different off set values of\nthis lower array, which remember, I had to", "start": 213.159, "duration": 4.371}
{"text": "flip around first, reveal all the distinct\npairs that have a given sum.", "start": 217.53, "duration": 7.69}
{"text": "As far as probability questions go, this still\nisn't especially interesting because all we're", "start": 225.22, "duration": 4.15}
{"text": "doing is counting how many outcomes there\nare in each of these categories.", "start": 229.37, "duration": 3.49}
{"text": "But that is with the implicit assumption that\nthere's an equal chance for each of these", "start": 232.86, "duration": 4.09}
{"text": "faces to come up.", "start": 236.95, "duration": 1.629}
{"text": "But what if I told you I have a special set\nof dice that's not uniform.", "start": 238.579, "duration": 3.731}
{"text": "Maybe the blue die has its own set of numbers\ndescribing the probabilities for each face", "start": 242.31, "duration": 3.68}
{"text": "coming up and the red die has its own unique\ndistinct set of numbers.", "start": 245.99, "duration": 3.969}
{"text": "In that case if you wanted to figure out,\nsay, the probability of seeing a two, you", "start": 249.959, "duration": 4.59}
{"text": "would multiply the probability that the blue\ndie is a 1X the probability that the red die", "start": 254.549, "duration": 4.631}
{"text": "is a one.", "start": 259.18, "duration": 1.04}
{"text": "And for the chances of seeing a three, you\nlook at the two distinct pairs where that's", "start": 260.22, "duration": 3.699}
{"text": "possible and again multiply the corresponding\nprobabilities and then add those two products", "start": 263.919, "duration": 5.411}
{"text": "together.", "start": 269.33, "duration": 1.0}
{"text": "Similarly, the chances of seeing a four involves\nmultiplying together three different pairs", "start": 270.33, "duration": 4.36}
{"text": "of possibilities and adding them all together.", "start": 274.69, "duration": 2.83}
{"text": "And in the spirit of setting up some formulas,\nlet's name these top probabilities \"A1, A2,", "start": 277.52, "duration": 4.25}
{"text": "A3\" and so on and name the bottom ones \"B1,\nB2, B3\" and so on.", "start": 281.77, "duration": 4.89}
{"text": "And in general, this process where we are\ntaking two different arrays of numbers, flipping", "start": 286.66, "duration": 4.22}
{"text": "the second one around and then lining them\nup at various different off set values, taking", "start": 290.88, "duration": 4.509}
{"text": "a bunch of pairwise products and adding them\nup, that's one of the fundamental ways to", "start": 295.389, "duration": 4.0}
{"text": "think about what a convolution is.", "start": 299.389, "duration": 3.811}
{"text": "So, just to spell it out a little more exactly-\nthrough this process, we just generated probabilities", "start": 303.2, "duration": 6.48}
{"text": "for seeing two, three, four, on and on up\nto 12.", "start": 309.68, "duration": 2.41}
{"text": "And we got them by mixing together one list\nof values, A and another list of values, B.", "start": 312.09, "duration": 5.46}
{"text": "In the lingo, we'd say the convolution of\nthose two sequences gives us this new sequence;", "start": 317.55, "duration": 5.58}
{"text": "the new sequence of 11 values, each of which\nlooks like some sum of pairwise products.", "start": 323.13, "duration": 4.75}
{"text": "If you prefer, another way you could think\nabout the same operation is to first create", "start": 327.88, "duration": 4.36}
{"text": "a table of all the pairwise product and then\nadd up along all these diagonals.", "start": 332.24, "duration": 4.95}
{"text": "Again, that's a way of mixing together these\ntwo sequences of numbers to get us a new sequence", "start": 337.19, "duration": 4.539}
{"text": "of eleven numbers.", "start": 341.729, "duration": 1.571}
{"text": "It's the same operation as the sliding windows\nthought, just another perspective.", "start": 343.3, "duration": 4.05}
{"text": "Putting a little notation to it, here's how\nyou might see it written down: The convolution", "start": 347.35, "duration": 3.69}
{"text": "of A and B, denoted with this little asterisk,\nis a new list.", "start": 351.04, "duration": 4.03}
{"text": "And the Nth element of that list looks like\na sum and that sum goes over all different", "start": 355.07, "duration": 5.189}
{"text": "pairs of indices, I and J, so that the sum\nof those indices is equal to N.", "start": 360.259, "duration": 4.921}
{"text": "It's kind of a mouthful but for example, if\nN was six, the pairs we're going over are", "start": 365.18, "duration": 4.77}
{"text": "one and five, two and four, three and three,\nfour and two, five and one.", "start": 369.95, "duration": 4.32}
{"text": "All the different pairs that add up to six.", "start": 374.27, "duration": 1.899}
{"text": "But honestly, however you write it down, the\nnotation is secondary in importance to the", "start": 376.169, "duration": 4.201}
{"text": "visual you might hold in your head for the\nprocess.", "start": 380.37, "duration": 2.76}
{"text": "Here, maybe it helps to do a super simple\nexample where I might ask you, what's the", "start": 383.13, "duration": 4.14}
{"text": "convolution of the list one, two, three with\nthe list four, five, six?", "start": 387.27, "duration": 4.269}
{"text": "You might picture taking both of these lists,\nflipping around that second one and then starting", "start": 391.539, "duration": 4.371}
{"text": "with its slid all the way over to the left.", "start": 395.91, "duration": 2.36}
{"text": "Then the pair of values which align are one\nand four, multiply them together and that", "start": 398.27, "duration": 3.72}
{"text": "gives us our first term of our output.", "start": 401.99, "duration": 2.21}
{"text": "Slide that bottom array one unit to the right,\nthe pairs which align are 1, 5, 2 and 4.", "start": 404.2, "duration": 5.469}
{"text": "Multiply those pairs, add them together and\nthat gives us 13.", "start": 409.669, "duration": 3.411}
{"text": "The next entry in our output.", "start": 413.08, "duration": 2.03}
{"text": "Slide things over once more and we'll take\n1X6+2X5+3X4 which happens to be 28.", "start": 415.11, "duration": 7.089}
{"text": "One more slide and we get 2X6+3X5 and that\ngives us 27 and finally the last term looks", "start": 422.199, "duration": 6.731}
{"text": "like 3X6.", "start": 428.93, "duration": 1.86}
{"text": "If you'd like, you can pull up whatever your\nfavorite programming languages and your favorite", "start": 430.79, "duration": 3.749}
{"text": "library that includes various numerical operations\nand you can confirm I'm not lying to you.", "start": 434.539, "duration": 4.541}
{"text": "If you take the convolution of 1, 2, 3 against\n4, 5, 6, this is indeed the result that you'll", "start": 439.08, "duration": 5.23}
{"text": "get.", "start": 444.31, "duration": 1.0}
{"text": "We've seen one case where this is a natural\nand desirable operation adding up to probability", "start": 445.31, "duration": 4.829}
{"text": "distributions.", "start": 450.139, "duration": 1.321}
{"text": "And another common example would be a moving\naverage.", "start": 451.46, "duration": 2.65}
{"text": "Imagine you have some long list of numbers\nand you take another smaller list of numbers", "start": 454.11, "duration": 4.5}
{"text": "that all add up to one.", "start": 458.61, "duration": 1.27}
{"text": "In this case, I just have a little list of\nfive values and they're all equal to one fifth.", "start": 459.88, "duration": 4.03}
{"text": "Then if we do this sliding window convolution\nprocess and kind of close our eyes and sweep", "start": 463.91, "duration": 5.45}
{"text": "under the rug what happens at the very beginning\nof it.", "start": 469.36, "duration": 2.559}
{"text": "Once our smaller list of values entirely overlaps\nwith the bigger one, think about what each", "start": 471.919, "duration": 4.59}
{"text": "term in this convolution really means.", "start": 476.509, "duration": 2.56}
{"text": "At each iteration, what you're doing is multiplying\neach of the values from your data by one fifth", "start": 479.069, "duration": 5.5}
{"text": "and adding them all together.", "start": 484.569, "duration": 1.78}
{"text": "Which is to say you're taking an average of\nyour data inside this little window.", "start": 486.349, "duration": 4.69}
{"text": "Overall, the process gives you a smooth out\nversion of the original data.", "start": 491.039, "duration": 4.341}
{"text": "And you could modify this starting with a\ndifferent little list of numbers and as long", "start": 495.38, "duration": 3.52}
{"text": "as that little list all adds up to one, you\ncan still interpret it as a moving average.", "start": 498.9, "duration": 3.989}
{"text": "In the example shown here, that moving average\nwould be giving more weight towards the central", "start": 502.889, "duration": 4.78}
{"text": "value.", "start": 507.669, "duration": 1.0}
{"text": "This also results in a smooth out version\nof the data.", "start": 508.669, "duration": 4.79}
{"text": "If you do kind of a two dimensional analog\nof this, it gives you a fun algorithm for", "start": 513.459, "duration": 4.02}
{"text": "blurring a given image.", "start": 517.479, "duration": 1.981}
{"text": "And I should say the animations I'm about\nto show are modified from something I originally", "start": 519.46, "duration": 4.31}
{"text": "made for part of a set of lectures I did with\nthe Julia Lab at MIT for a certain open courseware", "start": 523.77, "duration": 5.18}
{"text": "class that included an image processing unit.", "start": 528.95, "duration": 2.68}
{"text": "There we did a little bit more to dive into\nthe code behind all of this.", "start": 531.63, "duration": 2.92}
{"text": "So if you're curious, I'll leave you some\nlinks.", "start": 534.55, "duration": 2.15}
{"text": "But focusing back on this blurring example.", "start": 536.7, "duration": 2.21}
{"text": "What's going on is I've got this little 3X3\ngrid of values that marching along our original", "start": 538.91, "duration": 4.48}
{"text": "image.", "start": 543.39, "duration": 1.0}
{"text": "And if we zoom in, each one of those values\nis one ninth.", "start": 544.39, "duration": 2.66}
{"text": "And what I'm doing at each iteration is multiplying\neach of those values by the corresponding", "start": 547.05, "duration": 4.84}
{"text": "pixel that it sits on top of.", "start": 551.89, "duration": 2.05}
{"text": "And of course in computer science, we think\nof colors as little vectors of three values", "start": 553.94, "duration": 4.16}
{"text": "representing the red, green and blue components.", "start": 558.1, "duration": 2.57}
{"text": "When I multiply all these little values by\none ninth and I add them together, it gives", "start": 560.67, "duration": 3.97}
{"text": "us an average along each color channel and\nthe corresponding pixel for the image on the", "start": 564.64, "duration": 4.24}
{"text": "right is defined to be that sum.", "start": 568.88, "duration": 3.13}
{"text": "The overall effect as we do this for every\nsingle pixel on the image, is that each one", "start": 572.01, "duration": 4.26}
{"text": "kind of bleeds into all of its neighbours\nwhich gives us a blurrier version than the", "start": 576.27, "duration": 4.16}
{"text": "original.", "start": 580.43, "duration": 1.5}
{"text": "In the lingo, we'd say that the image on the\nright is a convolution of our original image", "start": 581.93, "duration": 4.719}
{"text": "with the little grid of values.", "start": 586.649, "duration": 1.661}
{"text": "Or more technically, maybe I should say that\nit's the convolution with a 180 degree rotated", "start": 588.31, "duration": 4.66}
{"text": "version of that little grid of values.", "start": 592.97, "duration": 1.99}
{"text": "Not that it matters when the grid is symmetric\nbut it's just worth keeping in mind that the", "start": 594.96, "duration": 3.51}
{"text": "definition of a convolution, as inherited\nfrom the pure math context, should always", "start": 598.47, "duration": 4.239}
{"text": "invite you to think about flipping around\nthat second array.", "start": 602.709, "duration": 3.261}
{"text": "If we modify this slightly, we can get a much\nmore elegant blurring effect by choosing a", "start": 605.97, "duration": 3.91}
{"text": "different grid of values.", "start": 609.88, "duration": 1.55}
{"text": "In this case, I have a little 5X5 grid but\nthe distinction is not so much its size.", "start": 611.43, "duration": 4.719}
{"text": "If we zoom in, we notice that the value in\nthe middle is a lot bigger than the value", "start": 616.149, "duration": 3.531}
{"text": "towards the edges.", "start": 619.68, "duration": 1.159}
{"text": "And where this is coming from, is they're\nall sampled from a bell curve known as a Gaussian", "start": 620.839, "duration": 4.731}
{"text": "distribution.", "start": 625.57, "duration": 1.0}
{"text": "That way, when we multiply all of these values\nby the corresponding pixel that they're on", "start": 626.57, "duration": 4.29}
{"text": "top of, we're giving a lot more weight to\nthat central pixel and much less towards the", "start": 630.86, "duration": 4.5}
{"text": "ones out at the edge.", "start": 635.36, "duration": 1.479}
{"text": "And just as before, the corresponding pixel\non the right is defined to be this sum.", "start": 636.839, "duration": 4.581}
{"text": "As we do this process for every single pixel,\nit gives a blurring effect which much more", "start": 641.42, "duration": 3.97}
{"text": "authentically simulates the notion of putting\nyour lens out of focus or something like that.", "start": 645.39, "duration": 4.61}
{"text": "But blurring is far from the only thing that\nyou can do with this idea.", "start": 650.0, "duration": 3.86}
{"text": "For instance, take a look at this little grid\nof values which involves some positive numbers", "start": 653.86, "duration": 4.1}
{"text": "on the left and some negative numbers on the\nright, which I'll color with blue and red", "start": 657.96, "duration": 4.35}
{"text": "respectively.", "start": 662.31, "duration": 1.42}
{"text": "Take a moment to see if you can predict and\nunderstand what effect this will have on the", "start": 663.73, "duration": 3.97}
{"text": "final image.", "start": 667.7, "duration": 3.06}
{"text": "So in this case, I'll just be thinking of\nthe image as grey scale instead of coloured.", "start": 670.76, "duration": 4.23}
{"text": "So each of the pixels is just represented\nby one number instead of three.", "start": 674.99, "duration": 3.529}
{"text": "And one thing worth noticing is that as we\ndo this convolution, it's possible to get", "start": 678.519, "duration": 3.661}
{"text": "negative values.", "start": 682.18, "duration": 1.0}
{"text": "For example, at this point here if we zoom\nin, the left half of our little grid sits", "start": 683.18, "duration": 4.09}
{"text": "entirely on top of black pixels which would\nhave a value of zero.", "start": 687.27, "duration": 3.69}
{"text": "But the right half of negative values all\nsit on top of white pixels which would have", "start": 690.96, "duration": 3.67}
{"text": "a value of one.", "start": 694.63, "duration": 1.63}
{"text": "So when we multiply corresponding terms and\nadd them together, the result will be very", "start": 696.26, "duration": 4.25}
{"text": "negative.", "start": 700.51, "duration": 1.0}
{"text": "And the way I'm displaying this with the image\non the right, is to color negative values", "start": 701.51, "duration": 3.16}
{"text": "red and positive values blue.", "start": 704.67, "duration": 2.46}
{"text": "Another thing to notice is that when you are\non a patch that's all the same color, everything", "start": 707.13, "duration": 3.51}
{"text": "goes to zero since the sum of the values in\nour little grid is zero.", "start": 710.64, "duration": 4.83}
{"text": "This is very different from the previous two\nexamples where the sum of our little grid", "start": 715.47, "duration": 3.0}
{"text": "was one, which let us interpret it as a moving\naverage and hence a blur.", "start": 718.47, "duration": 5.35}
{"text": "All in all, this little process basically\ndetects wherever there is variation in the", "start": 723.82, "duration": 4.18}
{"text": "pixel value as you move from left to right.", "start": 728.0, "duration": 2.38}
{"text": "And so it gives you a kind of way to pick\nup on all the vertical edges from your image.", "start": 730.38, "duration": 4.44}
{"text": "And similarly, if we rotated that grid around\nso that it varies as you move from the top", "start": 734.82, "duration": 6.0}
{"text": "to the bottom, this will be picking up on\nall the horizontal edges.", "start": 740.82, "duration": 3.769}
{"text": "Which in the case of our little pie creature\nimage, does result in some pretty demonic", "start": 744.589, "duration": 4.36}
{"text": "eyes.", "start": 748.949, "duration": 1.69}
{"text": "This smaller grid, by the way, is often called\na kernel.", "start": 750.639, "duration": 2.621}
{"text": "And the beauty here is how just by choosing\na different kernel, you can get different", "start": 753.26, "duration": 3.5}
{"text": "image processing effect.", "start": 756.76, "duration": 1.41}
{"text": "Not just blurring your edge detection but\nalso things like sharpening.", "start": 758.17, "duration": 3.0}
{"text": "For those of you who have heard of a convolutional\nneural network, the idea there is to use data", "start": 761.17, "duration": 4.229}
{"text": "to figure out what the kernels should be in\nthe first place.", "start": 765.399, "duration": 3.171}
{"text": "As determined by whatever the neural network\nwants to detect.", "start": 768.57, "duration": 4.39}
{"text": "Another thing I should maybe bring up is the\nlength of the output.", "start": 772.96, "duration": 2.62}
{"text": "For something like the moving average example,\nyou might only want to think about the terms", "start": 775.58, "duration": 3.97}
{"text": "when both of the windows fully align with\neach other.", "start": 779.55, "duration": 2.43}
{"text": "Or in the image processing example, maybe\nyou want the final output to have the same", "start": 781.98, "duration": 4.2}
{"text": "size as the original.", "start": 786.18, "duration": 1.56}
{"text": "Now, convolutions as a pure math operation,\nalways produce an array that's bigger than", "start": 787.74, "duration": 4.539}
{"text": "the two arrays that you started with.", "start": 792.279, "duration": 1.571}
{"text": "At least assuming one of them doesn't have\na length of one.", "start": 793.85, "duration": 3.01}
{"text": "Just know that in certain computer science\ncontext, you often want to deliberately truncate", "start": 796.86, "duration": 3.99}
{"text": "that output.", "start": 800.85, "duration": 4.18}
{"text": "Another thing worth highlighting is that in\nthe computer science context, this notion", "start": 805.03, "duration": 3.58}
{"text": "of flipping around that kernel before you\nlet it march across the original, often feels", "start": 808.61, "duration": 4.18}
{"text": "really weird and just uncalled for.", "start": 812.79, "duration": 2.48}
{"text": "But again, note that that's what's inherited\nfrom the pure math context, where like we", "start": 815.27, "duration": 3.89}
{"text": "saw with the probabilities, it's an incredibly\nnatural thing to do.", "start": 819.16, "duration": 3.989}
{"text": "And actually, I can show you one more pure\nmath example where even the programmers should", "start": 823.149, "duration": 3.951}
{"text": "care about this one because it opens the doors\nfor much faster algorithm to compute all of", "start": 827.1, "duration": 4.59}
{"text": "these.", "start": 831.69, "duration": 1.0}
{"text": "To set up what I mean by faster here, let\nme go back and pull up some Python again and", "start": 832.69, "duration": 4.49}
{"text": "I'm going to create two different relatively\nbig arrays.", "start": 837.18, "duration": 2.829}
{"text": "Each one will have 100,000 random elements\nin it and I'm going to assess the run time", "start": 840.009, "duration": 5.051}
{"text": "of the convolve function from the Numpy library.", "start": 845.06, "duration": 3.2}
{"text": "And in this case, it runs it for multiple\ndifferent iterations, tries to find an average", "start": 848.26, "duration": 3.85}
{"text": "and it looks like on this computer, at least,\nit averages at 4.87 seconds.", "start": 852.11, "duration": 4.45}
{"text": "By contrast, if I use a different function\nfrom the SciPy library called FFT convolve,", "start": 856.56, "duration": 6.029}
{"text": "which is the same just implemented differently,\nthat only takes 4.3 milliseconds on average.", "start": 862.589, "duration": 5.781}
{"text": "So, three orders of magnitude improvement.", "start": 868.37, "duration": 2.61}
{"text": "And again, even though it flies under a different\nname, it's giving the same output that the", "start": 870.98, "duration": 3.7}
{"text": "other convolve function does, it's just doing\nsomething to go about it in a cleverer way.", "start": 874.68, "duration": 5.86}
{"text": "[Music]", "start": 880.54, "duration": 1.729}
{"text": "- Remember how with the probability example,\nI said another way you could think about the", "start": 882.269, "duration": 4.06}
{"text": "convolution was to create this table of all\nthe pairwise products and then add up those", "start": 886.329, "duration": 4.391}
{"text": "pairwise products along the diagonals.", "start": 890.72, "duration": 2.25}
{"text": "There's, of course, nothing specific to probability.", "start": 892.97, "duration": 2.859}
{"text": "Anytime you're convolving two different lists\nof numbers, you can think about it this way.", "start": 895.829, "duration": 3.811}
{"text": "Create this kind of multiplication table with\nall Pairwise products and then each sum along", "start": 899.64, "duration": 4.119}
{"text": "the diagonal, corresponds to one of your final\noutputs.", "start": 903.759, "duration": 4.0}
{"text": "One context where this view is especially\nnatural, is when you multiply together two", "start": 907.759, "duration": 4.13}
{"text": "polynomials.", "start": 911.889, "duration": 1.091}
{"text": "For example, let me take the little grid we\nalready have and replace the top terms with", "start": 912.98, "duration": 3.94}
{"text": "1, 2X and 3X squared and replace the other\nterms with 4, 5X and 6X squared.", "start": 916.92, "duration": 7.15}
{"text": "Now, think about what it means when we're\ncreating all of these different pairwise products", "start": 924.07, "duration": 3.6}
{"text": "between the two lists.", "start": 927.67, "duration": 1.909}
{"text": "What you're doing is essentially expanding\nout the full product of the two polynomials", "start": 929.579, "duration": 4.13}
{"text": "I have written down.", "start": 933.709, "duration": 1.43}
{"text": "And then when you add up along the diagonal,\nthat corresponds to collecting all like terms.", "start": 935.139, "duration": 5.81}
{"text": "Which is pretty neat, expanding a polynomial\nand collecting like terms is exactly the same", "start": 940.949, "duration": 4.031}
{"text": "process as a convolution.", "start": 944.98, "duration": 3.089}
{"text": "But this allows us to do something that's\npretty cool.", "start": 948.069, "duration": 2.961}
{"text": "Because think about what we're saying here,\nwe're saying if you take two different functions", "start": 951.03, "duration": 4.03}
{"text": "and you multiply them together, which is a\nsimple point wise operation, that's the same", "start": 955.06, "duration": 4.449}
{"text": "thing as if you had first extracted the coefficients\nfrom each one of those, assuming they're polynomials,", "start": 959.509, "duration": 5.95}
{"text": "and then taking a convolution of those two\nlists of coefficients.", "start": 965.459, "duration": 4.241}
{"text": "What makes that so interesting is that convolutions\nfeel, in principle, a lot more complicated", "start": 969.7, "duration": 4.55}
{"text": "than simple multiplication.", "start": 974.25, "duration": 1.72}
{"text": "And I don't just mean conceptually they are\nharder to think about, I mean computationally", "start": 975.97, "duration": 4.05}
{"text": "it requires more steps to perform a convolution\nthan it does to perform a point wise product", "start": 980.02, "duration": 4.739}
{"text": "of two different lists.", "start": 984.759, "duration": 1.171}
{"text": "For example, let's say I gave you two really\nbig polynomials.", "start": 985.93, "duration": 3.82}
{"text": "Say each one with 100 different coefficients.", "start": 989.75, "duration": 3.05}
{"text": "Then if the way you multiply them was to expand\nout this product, you know, filling in this", "start": 992.8, "duration": 4.3}
{"text": "entire 100X100 grid of pairwise products.", "start": 997.1, "duration": 3.54}
{"text": "That would require you to perform 10,000 different\nproducts.", "start": 1000.64, "duration": 3.21}
{"text": "And then when you're collecting all the like\nterms along the diagonals, that's another", "start": 1003.85, "duration": 3.63}
{"text": "set of around 10,000 operations.", "start": 1007.48, "duration": 3.299}
{"text": "More generally in the lingo, we'd say the\nalgorithm is O of N squared.", "start": 1010.779, "duration": 4.04}
{"text": "Meaning for two lists of size N, the way that\nthe number of operations scales, is in proportion", "start": 1014.819, "duration": 5.041}
{"text": "to the square of N. On the other hand, if\nI think of two polynomials in terms of their", "start": 1019.86, "duration": 4.99}
{"text": "outputs, for example, sampling their values\nat some handful of inputs.", "start": 1024.85, "duration": 4.469}
{"text": "Then multiplying them only requires as many\noperations as the number of samples.", "start": 1029.319, "duration": 4.671}
{"text": "Since again it's a point wise operation and\nwith polynomials, you only need finitely mini", "start": 1033.99, "duration": 4.549}
{"text": "samples to be able to recover the coefficients.", "start": 1038.539, "duration": 2.04}
{"text": "For example, two outputs are enough to uniquely\nspecify a linear polynomial.", "start": 1040.579, "duration": 5.191}
{"text": "Three outputs would be enough to uniquely\nspecify a quadratic polynomial and in general,", "start": 1045.77, "duration": 4.899}
{"text": "if you know N distinct outputs, that's enough\nto uniquely specify a polynomial that has", "start": 1050.669, "duration": 4.831}
{"text": "N different coefficients.", "start": 1055.5, "duration": 2.059}
{"text": "Or if you prefer, we could phrase this in\nthe language of systems of equations.", "start": 1057.559, "duration": 3.711}
{"text": "Imagine I tell you, I have some polynomial\nbut I don't tell you what the coefficients", "start": 1061.27, "duration": 3.72}
{"text": "are, those are a mystery to you.", "start": 1064.99, "duration": 1.88}
{"text": "In our example, you might think of this as\nthe product that we're trying to figure out.", "start": 1066.87, "duration": 3.919}
{"text": "And then suppose I say, I'll just tell you\nwhat the outputs of this polynomial would", "start": 1070.789, "duration": 4.02}
{"text": "be if you inputted various different inputs\nlike 0, 1, 2, 3 on and on.", "start": 1074.809, "duration": 5.0}
{"text": "And I give you enough so that you have as\nmany equations as you have unknowns.", "start": 1079.809, "duration": 3.87}
{"text": "It even happens to be a linear system of equations\nso that's nice.", "start": 1083.679, "duration": 4.161}
{"text": "And in principle, at least, this should be\nenough to recover the coefficients.", "start": 1087.84, "duration": 3.23}
{"text": "So the rough algorithm outline then would\nbe whenever you want to convolve two lists", "start": 1091.07, "duration": 4.37}
{"text": "of numbers, you treat them like they're coefficients\nof two polynomials.", "start": 1095.44, "duration": 4.109}
{"text": "You sample those polynomials at enough outputs,\nmultiply those samples point wise, and then", "start": 1099.549, "duration": 6.341}
{"text": "solve the system to recover the coefficients\nas a sneaky backdoor way to find the convolution.", "start": 1105.89, "duration": 5.769}
{"text": "And as I've stated it so far, at least, some\nof you could rightfully complain, Grant, that", "start": 1111.659, "duration": 4.461}
{"text": "is an idiotic plan.", "start": 1116.12, "duration": 1.89}
{"text": "Because for one thing, just calculating all\nthese samples for one of the polynomials we", "start": 1118.01, "duration": 3.97}
{"text": "know, already takes on the order of N squared\noperations.", "start": 1121.98, "duration": 3.64}
{"text": "Not to mention, solving that system is certainly\ngoing to be computationally as difficult as", "start": 1125.62, "duration": 4.47}
{"text": "just doing the convolution in the first place.", "start": 1130.09, "duration": 2.5}
{"text": "So like, sure, we have this connection between\nmultiplication and convolutions but all of", "start": 1132.59, "duration": 4.66}
{"text": "the complexity happens in translating from\none viewpoint to the other.", "start": 1137.25, "duration": 4.52}
{"text": "But there is a trick.", "start": 1141.77, "duration": 1.519}
{"text": "And those of you who know about fourier transforms\nand the FFT algorithm, might see where this", "start": 1143.289, "duration": 3.941}
{"text": "is going.", "start": 1147.23, "duration": 1.1}
{"text": "If you're unfamiliar with these topics, what\nI'm about to say might seem completely out", "start": 1148.33, "duration": 3.38}
{"text": "of the blue.", "start": 1151.71, "duration": 1.0}
{"text": "Just know that there are certain paths you\ncould have walked in math that make this more", "start": 1152.71, "duration": 3.14}
{"text": "of an expected step.", "start": 1155.85, "duration": 1.69}
{"text": "Basically the idea is that we have a freedom\nof choice here.", "start": 1157.54, "duration": 3.04}
{"text": "If instead of evaluating as some arbitrary\nset of inputs like 0, 1, 2, 3 on and on, you", "start": 1160.58, "duration": 5.1}
{"text": "choose to evaluate on a very specially selected\nset of complex numbers.", "start": 1165.68, "duration": 4.95}
{"text": "Specifically the ones that sit evenly spaced\non the unit circle, what are known as the", "start": 1170.63, "duration": 3.43}
{"text": "roots of unity.", "start": 1174.06, "duration": 1.46}
{"text": "This gives us a friendlier system.", "start": 1175.52, "duration": 2.4}
{"text": "The basic idea is that by finding a number\nwhere taking its powers falls into this cycling", "start": 1177.92, "duration": 5.749}
{"text": "pattern, it means that the system we generate\nis going to have a lot of redundancy in the", "start": 1183.669, "duration": 4.281}
{"text": "different terms that you're calculating.", "start": 1187.95, "duration": 2.15}
{"text": "And by being clever about how you leverage\nthat redundancy, you can save yourself a lot", "start": 1190.1, "duration": 3.88}
{"text": "of work.", "start": 1193.98, "duration": 2.29}
{"text": "This set of outputs that I've written has\na special name.", "start": 1196.27, "duration": 2.5}
{"text": "It's called the discrete fourier transform\nof the coefficients.", "start": 1198.77, "duration": 4.139}
{"text": "And if you want to learn more, I actually\ndid another lecture for that same Julia MIT", "start": 1202.909, "duration": 4.14}
{"text": "class, all about discrete fourier transforms.", "start": 1207.049, "duration": 2.51}
{"text": "And there's also a really excellent video\non the channel reducible, talking about the", "start": 1209.559, "duration": 3.591}
{"text": "fast fourier transform, which is an algorithm\nfor computing these more quickly.", "start": 1213.15, "duration": 4.46}
{"text": "Also Veritasium recently did a really good\nvideo on FFTs so you've got lots of options.", "start": 1217.61, "duration": 4.83}
{"text": "And that fast algorithm really is the point\nfor us.", "start": 1222.44, "duration": 2.619}
{"text": "Again, because of all this redundancy, there\nexist a method to go from the coefficients", "start": 1225.059, "duration": 4.321}
{"text": "to all of these outputs.", "start": 1229.38, "duration": 1.77}
{"text": "Where instead of doing on the order of N squared\noperations, you do on the order of N times", "start": 1231.15, "duration": 4.29}
{"text": "the log of N operations which is much much\nbetter as you scale to big lists.", "start": 1235.44, "duration": 4.48}
{"text": "And importantly, this FFT algorithm goes both\nways.", "start": 1239.92, "duration": 2.93}
{"text": "It also lets you go from the outputs to the\ncoefficients.", "start": 1242.85, "duration": 2.809}
{"text": "Though bringing it all together, let's look\nback at our algorithm outline.", "start": 1245.659, "duration": 3.581}
{"text": "Now, we can say, whenever you're given two\nlong lists of numbers and you want to take", "start": 1249.24, "duration": 3.97}
{"text": "their convolution, first, compute the fast\nfourier transform of each one of them.", "start": 1253.21, "duration": 5.26}
{"text": "Which in the back of your mind, you can just\nthink of as treating them like they're the", "start": 1258.47, "duration": 3.22}
{"text": "coefficients of a polynomial and evaluating\nit at a very specially selected set of points.", "start": 1261.69, "duration": 5.27}
{"text": "Then, multiply together the two results that\nyou just got, point wise, which is nice and", "start": 1266.96, "duration": 4.42}
{"text": "fast.", "start": 1271.38, "duration": 1.0}
{"text": "And then do an inverse fast fourier transform\nand what that gives you is the sneaky back", "start": 1272.38, "duration": 4.039}
{"text": "door way to compute the convolution that we\nwere looking for.", "start": 1276.419, "duration": 2.901}
{"text": "But this time, it only involves O of N log\nN operations.", "start": 1279.32, "duration": 4.15}
{"text": "That's really cool to me.", "start": 1283.47, "duration": 1.74}
{"text": "This very specific context where convolutions\nshow up, multiplying two polynomials, opens", "start": 1285.21, "duration": 4.92}
{"text": "the doors for an algorithm that's relevant\neverywhere else where convolutions might come", "start": 1290.13, "duration": 3.73}
{"text": "up.", "start": 1293.86, "duration": 1.0}
{"text": "If you want to add probability distributions\nto some large image processing, whatever it", "start": 1294.86, "duration": 3.66}
{"text": "might be.", "start": 1298.52, "duration": 1.0}
{"text": "And I just think that's such a good example\nof why you should be excited when you see", "start": 1299.52, "duration": 3.19}
{"text": "some operation or concept in math show up\nin a lot of seemingly unrelated areas.", "start": 1302.71, "duration": 6.24}
{"text": "If you want a little homework, here's something\nthat's fun to think about.", "start": 1308.95, "duration": 2.9}
{"text": "Explain why when you multi two different numbers,\njust ordinary multiplication the way we all", "start": 1311.85, "duration": 4.76}
{"text": "learn in elementary school, what you're doing\nis basically a convolution between the digits", "start": 1316.61, "duration": 4.549}
{"text": "of those numbers.", "start": 1321.159, "duration": 1.51}
{"text": "There's some added steps with carries and\nthe like but the core step is a convolution.", "start": 1322.669, "duration": 4.791}
{"text": "In light of the existence of a fast algorithm,\nwhat that means is if you have two very large", "start": 1327.46, "duration": 4.959}
{"text": "integers, then there exist a way to find their\nproduct that's faster than the method we learn", "start": 1332.419, "duration": 4.591}
{"text": "in elementary school.", "start": 1337.01, "duration": 1.2}
{"text": "That instead of requiring O of N squared operations,\nonly requires O of N log N. Which doesn't", "start": 1338.21, "duration": 5.31}
{"text": "even feel like it should be possible.", "start": 1343.52, "duration": 1.97}
{"text": "The catch is that before this is actually\nuseful in practice, your numbers would have", "start": 1345.49, "duration": 3.86}
{"text": "to be absolutely monstrous.", "start": 1349.35, "duration": 2.3}
{"text": "But still it's cool that such an algorithm\nexists.", "start": 1351.65, "duration": 3.66}
{"text": "And next up, will turn our attention to the\ncontinuous case with the special focus on", "start": 1355.31, "duration": 3.729}
{"text": "probability distributions.", "start": 1359.039, "duration": 1.571}
